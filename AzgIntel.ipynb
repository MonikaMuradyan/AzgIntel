{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets accelerate evaluate\n"
      ],
      "metadata": {
        "id": "PUUdsVfhXFc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-itpTlOdl4p8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "uploaded = files.upload()\n",
        "file_name = next(iter(uploaded))\n",
        "df = pd.read_excel(io.BytesIO(uploaded[file_name]))\n",
        "df.to_csv(\"armenian_propaganda_dataset.csv\", index=False)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AZds_SGAoDO-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['cleaned_text'], df['label'], test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rXh3JM3oJaN"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vec, y_train)\n",
        "y_pred = model.predict(X_test_vec)\n",
        "print(f\"’É’∑’£÷Ä’ø’∏÷Ç’©’µ’∏÷Ç’∂: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred, target_names=[\"’â’•’¶’∏÷Ñ (0)\", \"’î’°÷Ä’∏’¶’π’°’Ø’°’∂ (1)\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NeSj82LoY0H"
      },
      "outputs": [],
      "source": [
        "def predict_propaganda(text):\n",
        "    text_cleaned = clean_text(text)\n",
        "    text_vec = vectorizer.transform([text_cleaned])\n",
        "    prediction = model.predict(text_vec)[0]\n",
        "    return \"’î’°÷Ä’∏’¶’π’°’Ø’°’∂ (1)\" if prediction == 1 else \"’â’•’¶’∏÷Ñ (0)\"\n",
        "\n",
        "print(predict_propaganda(\"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’°’∂’∞÷Ä’°’™’•’∑’ø ’ß\"))\n",
        "print(predict_propaganda(\"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å AI ’∞’°’Ø’∏’∂\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2YyTcgORo8XY"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_vec = vectorizer.fit_transform(df['cleaned_text'])\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(X_vec, df['label'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8LeUNIbpAIo"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdqIAKDipHwn"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "selector = SelectKBest(chi2, k=500)\n",
        "X_new = selector.fit_transform(X_resampled, y_resampled)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_new, y_resampled, test_size=0.2, random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqGwm94opO3n"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "model = XGBClassifier(\n",
        "    scale_pos_weight=sum(y_resampled==0)/sum(y_resampled==1),\n",
        "    eval_metric='logloss'\n",
        ")\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUx0GODKpo-5"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)\n",
        "model = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1,\n",
        "    scale_pos_weight=scale_pos_weight,\n",
        "    eval_metric='logloss',\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=[\"’â’•’¶’∏÷Ñ (0)\", \"’î’°÷Ä’∏’¶’π’°’Ø’°’∂ (1)\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUvWp02tqKBg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "y_pred_optimized = (y_probs >= optimal_threshold).astype(int)\n",
        "print(classification_report(y_test, y_pred_optimized))\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [100, 200, 300]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(XGBClassifier(), param_grid, cv=3, scoring='f1')\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"‘º’°’æ’°’£’∏÷Ç’µ’∂ ’∫’°÷Ä’°’¥’•’ø÷Ä’•÷Ä:\", grid.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0ciZZgjqaa6"
      },
      "outputs": [],
      "source": [
        "!pip install catboost\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    class_weight='balanced',\n",
        "    n_estimators=200,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"Random Forest Results:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "cb = CatBoostClassifier(\n",
        "    auto_class_weights='Balanced',\n",
        "    iterations=200,\n",
        "    depth=5,\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "cb.fit(X_train, y_train)\n",
        "y_pred_cb = cb.predict(X_test)\n",
        "print(\"\\nCatBoost Results:\")\n",
        "print(classification_report(y_test, y_pred_cb))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcIyT0j5r6pq"
      },
      "outputs": [],
      "source": [
        "cb = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    depth=6,\n",
        "    learning_rate=0.05,\n",
        "    auto_class_weights='Balanced',\n",
        "    verbose=0\n",
        ")\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import joblib\n",
        "joblib.dump(cb, 'best_propaganda_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcCPjYHxs3mu"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "import joblib\n",
        "cb = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    depth=6,\n",
        "    learning_rate=0.05,\n",
        "    auto_class_weights='Balanced',\n",
        "    verbose=0\n",
        ")\n",
        "cb.fit(X_train, y_train)\n",
        "joblib.dump({'model': cb, 'vectorizer': vectorizer}, 'propaganda_model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4f8fvEbtV9Y"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "saved_data = joblib.load('propaganda_model.pkl')\n",
        "loaded_model = saved_data['model']\n",
        "vectorizer = saved_data['vectorizer']\n",
        "\n",
        "def predict_text(text):\n",
        "    cleaned_text = clean_text(text)\n",
        "    text_vec = vectorizer.transform([cleaned_text])\n",
        "    prediction = loaded_model.predict(text_vec)[0]\n",
        "    return \"’î’°÷Ä’∏’¶’π’°’Ø’°’∂\" if prediction == 1 else \"’â’•’¶’∏÷Ñ\"\n",
        "print(predict_text(\"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’∫’•’ø÷Ñ ’ß ’§’°’º’∂’° ’°’º’°’ª’∂’°’∞’•÷Ä’©’∏÷Ç’©’µ’∏÷Ç’∂\"))\n",
        "print(predict_text(\"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’ø’•’≠’∂’∏’¨’∏’£’´’°’Ø’°’∂ ÷É’°’º’°’ø’∏’∂\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgzQf-jWte_o"
      },
      "outputs": [],
      "source": [
        "cb.save_model('catboost_model.cbm')\n",
        "from catboost import CatBoost\n",
        "loaded_model = CatBoost()\n",
        "loaded_model.load_model('catboost_model.cbm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAKsDBIDtgph"
      },
      "outputs": [],
      "source": [
        "print(f\"’Ñ’∏’§’•’¨’® ’∏÷Ç’Ω’∏÷Ç÷Å’æ’°’Æ ’ß: {loaded_model.is_fitted()}\")\n",
        "print(f\"’ï’£’ø’°’£’∏÷Ä’Æ’æ’°’Æ ’∞’°’ø’Ø’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’´ ÷Ñ’°’∂’°’Ø: {loaded_model.feature_names_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-o8AImht-5E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier\n",
        "import joblib\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
        "    df['cleaned_text'],\n",
        "    df['label'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=500)\n",
        "X_train_vec = vectorizer.fit_transform(X_train_texts)\n",
        "X_test_vec = vectorizer.transform(X_test_texts)\n",
        "cb = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    depth=6,\n",
        "    learning_rate=0.05,\n",
        "    auto_class_weights='Balanced',\n",
        "    verbose=0\n",
        ")\n",
        "cb.fit(X_train_vec, y_train)\n",
        "joblib.dump({'model': cb, 'vectorizer': vectorizer}, 'propaganda_model_full.pkl')\n",
        "def predict_text(text):\n",
        "    cleaned_text = clean_text(text)\n",
        "    text_vec = vectorizer.transform([cleaned_text])\n",
        "    prediction = cb.predict(text_vec)[0]\n",
        "    return \"’î’°÷Ä’∏’¶’π’°’Ø’°’∂\" if prediction == 1 else \"’â’•’¶’∏÷Ñ\"\n",
        "\n",
        "print(predict_text(\"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’∫’•’ø÷Ñ ’ß ’§’°’º’∂’° ’°’º’°’ª’∂’°’∞’•÷Ä’©’∏÷Ç’©’µ’∏÷Ç’∂\"))\n",
        "print(predict_text(\"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’ø’•’≠’∂’∏’¨’∏’£’´’°’Ø’°’∂ ÷É’°’º’°’ø’∏’∂\"))\n",
        "feature_importances = cb.get_feature_importance()\n",
        "top_features = sorted(zip(vectorizer.get_feature_names_out(), feature_importances),\n",
        "                 key=lambda x: x[1], reverse=True)[:10]\n",
        "print(\"Top Features:\", top_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDUgY08fvpct"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class_weights = [1, len(y_train[y_train==0])/len(y_train[y_train==1])]\n",
        "\n",
        "cb = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    depth=5,\n",
        "    learning_rate=0.05,\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='F1',\n",
        "    early_stopping_rounds=20,\n",
        "    class_weights=class_weights,\n",
        "    verbose=100\n",
        ")\n",
        "\n",
        "cb.fit(X_train_vec, y_train, eval_set=(X_test_vec, y_test))\n",
        "\n",
        "y_pred = cb.predict(X_test_vec)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['’â’•’¶’∏÷Ñ (0)', '’î’°÷Ä’∏’¶’π’°’Ø’°’∂ (1)'],\n",
        "            yticklabels=['’â’•’¶’∏÷Ñ (0)', '’î’°÷Ä’∏’¶’π’°’Ø’°’∂ (1)'])\n",
        "plt.xlabel('‘ø’°’∂’≠’°’ø’•’Ω’æ’°’Æ')\n",
        "plt.ylabel('’ì’°’Ω’ø’°÷Å’´')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "wrong_indices = np.where(y_test != y_pred)[0][:5]\n",
        "\n",
        "for i, idx in enumerate(wrong_indices):\n",
        "    print(f\"\\n’ï÷Ä’´’∂’°’Ø {i+1}:\")\n",
        "    print(f\"’è’•÷Ñ’Ω’ø: {X_test_texts.iloc[idx]}\")\n",
        "    print(f\"‘ø’°’∂’≠’°’ø’•’Ω’∏÷Ç’¥: {'’î’°÷Ä’∏’¶’π’°’Ø’°’∂' if y_pred[idx] == 1 else '’â’•’¶’∏÷Ñ'}\")\n",
        "    print(f\"’ì’°’Ω’ø’°÷Å’´: {'’î’°÷Ä’∏’¶’π’°’Ø’°’∂' if y_test.iloc[idx] == 1 else '’â’•’¶’∏÷Ñ'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggPO4YkkxtRM"
      },
      "outputs": [],
      "source": [
        "!pip install nlpaug\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77ixUcNMy7U0"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XflgB1Oay-Hg"
      },
      "outputs": [],
      "source": [
        "!pip install nlpaug transformers\n",
        "!python -m nltk.downloader wordnet omw-1.4\n",
        "\n",
        "import random\n",
        "from nlpaug.augmenter.word import SynonymAug\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "def armenian_augment(text, n=2):\n",
        "    \"\"\"Custom augmentation for Armenian text\"\"\"\n",
        "    try:\n",
        "        aug = SynonymAug(\n",
        "            aug_src='wordnet',\n",
        "            lang='hye',\n",
        "            aug_p=0.3,\n",
        "            stopwords=['÷á', '’ß', '’®', '’∏÷Ä', '’´’∂’π']\n",
        "        )\n",
        "        return [aug.augment(text) for _ in range(n)]\n",
        "    except:\n",
        "        replacements = {\n",
        "            \"÷Ö÷Ä’´’∂’°’Ø\": [\"’∂’¥’∏÷Ç’∑\", \"’∫’°’ø’Ø’•÷Ä\"],\n",
        "            \"’ø’•÷Ñ’Ω’ø\": [\"’£÷Ä’°’º’∏÷Ç’¥\", \"’∞’∏’§’æ’°’Æ\"],\n",
        "            \"’Ä‘±’ä‘ø\": [\"’Ä’°’µ’°’Ω’ø’°’∂’´ ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’°’∂ ’∫’°’µ’¥’°’∂’°’£’´÷Ä\", \"’º’°’¶’¥’°’Ø’°’∂ ’§’°’∑’´’∂÷Ñ\"],\n",
        "            \"’°’§÷Ä’¢’•’ª’°’∂\": [\"’°’§÷Ä’¢’•’ª’°’∂’°’Ø’°’∂ ’Ø’∏’≤’¥\", \"’∞’°÷Ä÷á’°’∂ ’•÷Ä’Ø’´÷Ä\"]\n",
        "        }\n",
        "        results = []\n",
        "        for _ in range(n):\n",
        "            augmented = text\n",
        "            for word, subs in replacements.items():\n",
        "                if word in augmented:\n",
        "                    augmented = augmented.replace(word, random.choice(subs))\n",
        "            results.append(augmented)\n",
        "        return results\n",
        "\n",
        "print(armenian_augment(\"÷Ö÷Ä’´’∂’°’Ø ’ø’•÷Ñ’Ω’ø ’Ä‘±’ä‘ø-’´ ’¥’°’Ω’´’∂\", n=2))\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=1000,\n",
        "    ngram_range=(1,2),\n",
        "    token_pattern=r'\\b[’°-÷Ü‘±-’ñ]+\\b'\n",
        ")\n",
        "\n",
        "cb = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    depth=4,\n",
        "    learning_rate=0.03,\n",
        "    l2_leaf_reg=5,\n",
        "    early_stopping_rounds=10,\n",
        "    eval_metric='F1',\n",
        "    class_weights=[1, 3],\n",
        "    verbose=100\n",
        ")\n",
        "\n",
        "try:\n",
        "    from transformers import AutoTokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Geotrend/bert-base-hy\")\n",
        "    print(\"BERT tokenizer loaded successfully for Armenian\")\n",
        "except:\n",
        "    print(\"Proceeding with CatBoost only\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aNNfMAt1HEQ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from nlpaug.augmenter.word import SynonymAug\n",
        "def armenian_augment(text, n=2):\n",
        "    \"\"\"Improved augmentation with Armenian-specific rules\"\"\"\n",
        "    armenian_synonyms = {\n",
        "        \"’Ä‘±’ä‘ø\": [\"’Ä’°’µ’°’Ω’ø’°’∂’´ ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’°’∂ ’∫’°’µ’¥’°’∂’°’£’´÷Ä\", \"’º’°’¶’¥’°’Ø’°’∂ ’§’°’∑’´’∂÷Ñ\", \"’Ä‘±’ä‘ø ’Ø’°’¶’¥’°’Ø’•÷Ä’∫’∏÷Ç’©’µ’∏÷Ç’∂\"],\n",
        "        \"’°’§÷Ä’¢’•’ª’°’∂\": [\"’°’§÷Ä’¢’•’ª’°’∂’°’Ø’°’∂ ’Ø’∏’≤’¥\", \"’∞’°÷Ä÷á’°’∂ ’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂\", \"‘±’§÷Ä’¢’•’ª’°’∂’´ ’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂\"],\n",
        "        \"÷É’°’∑’´’∂’µ’°’∂\": [\"’æ’°÷Ä’π’°’∫’•’ø\", \"’∂’´’Ø’∏’¨ ÷É’°’∑’´’∂’µ’°’∂\", \"’Ø’°’º’°’æ’°÷Ä’∏÷Ç’©’µ’°’∂ ’≤’•’Ø’°’æ’°÷Ä\"],\n",
        "        \"’¶’´’∂’æ’°’Æ\": [\"’º’°’¶’¥’°’Ø’°’∂\", \"’∫’°’∑’ø’∫’°’∂’°’Ø’°’∂\", \"’¶’´’∂’æ’∏÷Ä’°’Ø’°’∂\"],\n",
        "        \"’∞’°÷Ä’±’°’Ø’∏÷Ç’¥\": [\"’°’£÷Ä’•’Ω’´’°\", \"’∞÷Ä’°’±’£’∏÷Ç’©’µ’∏÷Ç’∂\", \"’¶’´’∂’æ’°’Æ ’¢’°’≠’∏÷Ç’¥\"]\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    for _ in range(n):\n",
        "        augmented = text\n",
        "        for word, subs in armenian_synonyms.items():\n",
        "            if word in augmented:\n",
        "                augmented = augmented.replace(word, random.choice(subs))\n",
        "        results.append(augmented)\n",
        "    return results\n",
        "\n",
        "print(armenian_augment(\"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’∏÷Ç ’¥’°’Ω’´’∂ ’∞’°’µ’ø’°÷Ä’°÷Ä’∏÷Ç’©’µ’∏÷Ç’∂\", n=2))\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=1500,\n",
        "    ngram_range=(1, 3),\n",
        "    token_pattern=r'\\b[’°-÷Ü‘±-’ñ]{3,}\\b',\n",
        "    stop_words=['÷á', '’ß', '’®', '’∏÷Ä', '’´’∂’π', '’∫’•’ø÷Ñ', '’ß÷Ä']\n",
        ")\n",
        "cb = CatBoostClassifier(\n",
        "    iterations=800,\n",
        "    depth=5,\n",
        "    learning_rate=0.025,\n",
        "    l2_leaf_reg=3,\n",
        "    early_stopping_rounds=15,\n",
        "    eval_metric='F1',\n",
        "    class_weights=[1, 4],\n",
        "    text_features=['text'],\n",
        "    verbose=200\n",
        ")\n",
        "try:\n",
        "    from transformers import pipeline\n",
        "    bert_classifier = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=\"Geotrend/bert-base-hy\",\n",
        "        tokenizer=\"Geotrend/bert-base-hy\"\n",
        "    )\n",
        "    print(\"BERT model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Couldn't load BERT model: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWnWaPvd1fzP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from nlpaug.augmenter.word import SynonymAug\n",
        "\n",
        "def armenian_augment(text, n=2):\n",
        "    \"\"\"Improved augmentation with varied outputs\"\"\"\n",
        "    armenian_synonyms = {\n",
        "        \"’Ä‘±’ä‘ø\": [\"’Ä’°’µ’°’Ω’ø’°’∂’´ ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’°’∂ ’∫’°’µ’¥’°’∂’°’£’´÷Ä\", \"’º’°’¶’¥’°’Ø’°’∂ ’§’°’∑’´’∂÷Ñ\", \"’Ä‘±’ä‘ø ’Ø’°’¶’¥’°’Ø’•÷Ä’∫’∏÷Ç’©’µ’∏÷Ç’∂\"],\n",
        "        \"’°’§÷Ä’¢’•’ª’°’∂\": [\"’°’§÷Ä’¢’•’ª’°’∂’°’Ø’°’∂ ’Ø’∏’≤’¥\", \"’∞’°÷Ä÷á’°’∂ ’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂\", \"‘±’§÷Ä’¢’•’ª’°’∂’´ ’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂\"],\n",
        "        \"÷É’°’∑’´’∂’µ’°’∂\": [\"’æ’°÷Ä’π’°’∫’•’ø\", \"’∂’´’Ø’∏’¨ ÷É’°’∑’´’∂’µ’°’∂\", \"’Ø’°’º’°’æ’°÷Ä’∏÷Ç’©’µ’°’∂ ’≤’•’Ø’°’æ’°÷Ä\"],\n",
        "        \"’¶’´’∂’æ’°’Æ\": [\"’º’°’¶’¥’°’Ø’°’∂\", \"’∫’°’∑’ø’∫’°’∂’°’Ø’°’∂\", \"’¶’´’∂’æ’∏÷Ä’°’Ø’°’∂\"],\n",
        "        \"’∞’°÷Ä’±’°’Ø’∏÷Ç’¥\": [\"’°’£÷Ä’•’Ω’´’°\", \"’∞÷Ä’°’±’£’∏÷Ç’©’µ’∏÷Ç’∂\", \"’¶’´’∂’æ’°’Æ ’¢’°’≠’∏÷Ç’¥\"],\n",
        "        \"’§’∏÷Ç÷Ä’Ω ’£’°’¨\": [\"’∞’•’º’°’∂’°’¨\", \"’¨÷Ñ’•’¨\", \"’§’°’§’°÷Ä’•’¨ ’¥’°’Ω’∂’°’Ø÷Å’•’¨\"]\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    words = text.split()\n",
        "    for _ in range(n):\n",
        "        augmented = []\n",
        "        for word in words:\n",
        "            if word in armenian_synonyms and random.random() < 0.5:\n",
        "                augmented.append(random.choice(armenian_synonyms[word]))\n",
        "            else:\n",
        "                augmented.append(word)\n",
        "        results.append(' '.join(augmented))\n",
        "    return results\n",
        "print(armenian_augment(\"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’∏÷Ç ’¥’°’Ω’´’∂ ’∞’°’µ’ø’°÷Ä’°÷Ä’∏÷Ç’©’µ’∏÷Ç’∂\", n=3))\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=1500,\n",
        "    ngram_range=(1, 3),\n",
        "    token_pattern=r'\\b[’°-÷Ü‘±-’ñ]{3,}\\b',\n",
        "    stop_words=['÷á', '’ß', '’®', '’∏÷Ä', '’´’∂’π', '’∫’•’ø÷Ñ', '’ß÷Ä']\n",
        ")\n",
        "\n",
        "cb = CatBoostClassifier(\n",
        "    iterations=800,\n",
        "    depth=5,\n",
        "    learning_rate=0.025,\n",
        "    l2_leaf_reg=3,\n",
        "    early_stopping_rounds=15,\n",
        "    eval_metric='F1',\n",
        "    class_weights=[1, 4],\n",
        "    verbose=200\n",
        ")\n",
        "try:\n",
        "    from transformers import pipeline\n",
        "    bert_classifier = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=\"bert-base-multilingual-cased\",\n",
        "        tokenizer=\"bert-base-multilingual-cased\"\n",
        "    )\n",
        "    print(\"Multilingual BERT model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Couldn't load BERT model: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = {\n",
        "    'text': [\n",
        "        '’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’ß',\n",
        "        '‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’ø’•’≠’∂’∏’¨’∏’£’´’°’Ø’°’∂ ÷É’°’º’°’ø’∏’∂',\n",
        "        '’ì’°’∑’´’∂’µ’°’∂’´ ’∞’°’µ’ø’°÷Ä’°÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® ’æ’°’ø’∂ ’ß',\n",
        "        '‘¥’∫÷Ä’∏÷Å’∂’•÷Ä’∏÷Ç’¥ ’∂’∏÷Ä ’Æ÷Ä’°’£’´÷Ä ’ß ’¥’ø÷Å’æ’•’¨'\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "def armenian_augment(text, n=3):\n",
        "    synonyms = {\n",
        "        \"’Ä‘±’ä‘ø\": [\"’Ä’°’µ’°’Ω’ø’°’∂’´ ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’°’∂ ’∫’°’µ’¥’°’∂’°’£’´÷Ä\", \"’º’°’¶’¥’°’Ø’°’∂ ’§’°’∑’´’∂÷Ñ\"],\n",
        "        \"÷É’°’∑’´’∂’µ’°’∂\": [\"’æ’°÷Ä’π’°’∫’•’ø\", \"’∂’´’Ø’∏’¨ ÷É’°’∑’´’∂’µ’°’∂\"],\n",
        "        \"’§’∏÷Ç÷Ä’Ω ’£’°’¨\": [\"’∞’•’º’°’∂’°’¨\", \"’¨÷Ñ’•’¨\"]\n",
        "    }\n",
        "    results = []\n",
        "    for _ in range(n):\n",
        "        words = text.split()\n",
        "        augmented = [random.choice(synonyms.get(word, [word])) for word in words]\n",
        "        results.append(' '.join(augmented))\n",
        "    return results\n",
        "\n",
        "print(\"‘±÷Ç’£’¥’•’∂’ø’°÷Å’´’°’µ’´ ÷Ö÷Ä’´’∂’°’Ø:\")\n",
        "augmented = armenian_augment(\"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’ß\", n=2)\n",
        "for i, text in enumerate(augmented, 1):\n",
        "    print(f\"{i}. {text}\")\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=1000,\n",
        "    ngram_range=(1, 2),\n",
        "    stop_words=['÷á', '’ß', '’®', '’∏÷Ä']\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.25)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "cb = CatBoostClassifier(\n",
        "    iterations=300,\n",
        "    depth=4,\n",
        "    learning_rate=0.05,\n",
        "    early_stopping_rounds=10,\n",
        "    verbose=50\n",
        ")\n",
        "\n",
        "print(\"\\n’Ñ’∏’§’•’¨’´ ’∏÷Ç’Ω’∏÷Ç÷Å’∏÷Ç’¥ ’Ω’Ø’Ω’æ’°’Æ ’ß...\")\n",
        "cb.fit(X_train_vec, y_train)\n",
        "print(\"’Ñ’∏’§’•’¨’´ ’∏÷Ç’Ω’∏÷Ç÷Å’∏÷Ç’¥ ’°’æ’°÷Ä’ø’æ’°’Æ ’ß\")\n",
        "\n",
        "def save_models_fast():\n",
        "    joblib.dump({\n",
        "        'model': cb,\n",
        "        'vectorizer': vectorizer\n",
        "    }, 'fast_model.pkl', compress=3)\n",
        "    print(\"\\n’Ñ’∏’§’•’¨’® ’∞’°’ª’∏’≤’∏÷Ç’©’µ’°’¥’¢ ’∫’°’∞’∫’°’∂’æ’°’Æ ’ß fast_model.pkl ÷Ü’°’µ’¨’∏÷Ç’¥\")\n",
        "\n",
        "save_models_fast()\n",
        "\n",
        "test_text = \"’Ä‘±’ä‘ø-’´÷Å ’∞’•’º’°’∂’°’¨’® ’æ’°’ø ’£’°’≤’°÷É’°÷Ä ’ß\"\n",
        "test_vec = vectorizer.transform([test_text])\n",
        "prediction = cb.predict(test_vec)[0]\n",
        "print(f\"\\n’ì’∏÷Ä’±’°÷Ä’Ø’∏÷Ç’¥’ù '{test_text}'\")\n",
        "print(\"‘ø’°’∂’≠’°’ø’•’Ω’∏÷Ç’¥:\", \"’î’°÷Ä’∏’¶’π’°’Ø’°’∂\" if prediction == 1 else \"’â’•’¶’∏÷Ñ\")"
      ],
      "metadata": {
        "id": "1GnPh4acR1lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "data = {\n",
        "    'text': [\n",
        "        '’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’ß',\n",
        "        '‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’ø’•’≠’∂’∏’¨’∏’£’´’°’Ø’°’∂ ÷É’°’º’°’ø’∏’∂',\n",
        "        '’ì’°’∑’´’∂’µ’°’∂’´ ’∞’°’µ’ø’°÷Ä’°÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’ß',\n",
        "        '‘¥’∫÷Ä’∏÷Å’∂’•÷Ä’∏÷Ç’¥ ’∂’∏÷Ä ’Æ÷Ä’°’£’´÷Ä ’ß ’¥’ø÷Å’æ’•’¨',\n",
        "        '‘±’§÷Ä’¢’•’ª’°’∂’® ’≠’°’≠’ø’∏÷Ç’¥ ’ß ’∞÷Ä’°’§’°’§’°÷Ä’®',\n",
        "        '’Ä’°’µ’°’Ω’ø’°’∂’∏÷Ç’¥ ’ø’∂’ø’•’Ω’°’Ø’°’∂ ’°’≥ ’ß ’£÷Ä’°’∂÷Å’æ’∏÷Ç’¥',\n",
        "        '’å’°’¶’¥’°’Ø’°’∂ ’§’°’∑’´’∂÷Ñ’® ’∫’•’ø÷Ñ ’ß ’¨’∏÷Ç’Æ’°÷Ä’æ’´',\n",
        "        '‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’¥’∑’°’Ø’∏÷Ç’©’°’µ’´’∂ ’¥’´’ª’∏÷Å’°’º’∏÷Ç’¥'\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 1, 0, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "def armenian_augment(text, n=3):\n",
        "    synonyms = {\n",
        "        \"’Ä‘±’ä‘ø\": [\"’Ä’°’µ’°’Ω’ø’°’∂’´ ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’°’∂ ’∫’°’µ’¥’°’∂’°’£’´÷Ä\", \"’º’°’¶’¥’°’Ø’°’∂ ’§’°’∑’´’∂÷Ñ\", \"’Ä‘±’ä‘ø ’Ø’°’¶’¥’°’Ø’•÷Ä’∫’∏÷Ç’©’µ’∏÷Ç’∂\"],\n",
        "        \"’°’§÷Ä’¢’•’ª’°’∂\": [\"’°’§÷Ä’¢’•’ª’°’∂’°’Ø’°’∂ ’Ø’∏’≤’¥\", \"’∞’°÷Ä÷á’°’∂ ’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂\", \"‘±’§÷Ä’¢’•’ª’°’∂’´ ’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂\"],\n",
        "        \"÷É’°’∑’´’∂’µ’°’∂\": [\"’æ’°÷Ä’π’°’∫’•’ø’®\", \"’∂’´’Ø’∏’¨ ÷É’°’∑’´’∂’µ’°’∂\", \"’Ø’°’º’°’æ’°÷Ä’∏÷Ç’©’µ’°’∂ ’≤’•’Ø’°’æ’°÷Ä’®\"],\n",
        "        \"’æ’ø’°’∂’£’°’æ’∏÷Ä\": [\"’æ’∂’°’Ω’°’Ø’°÷Ä\", \"’Ω’∫’°’º’∂’°’¨’´÷Å\", \"’æ’ø’°’∂’£ ’∂’•÷Ä’Ø’°’µ’°÷Å’∂’∏’≤\"],\n",
        "        \"’§’∏÷Ç÷Ä’Ω ’£’°’¨\": [\"’∞’•’º’°’∂’°’¨\", \"’¨÷Ñ’•’¨\", \"’§’°’§’°÷Ä’•’¨ ’¥’°’Ω’∂’°’Ø÷Å’•’¨\"],\n",
        "        \"’¨’∏÷Ç’Æ’°÷Ä’æ’´\": [\"’§’°’§’°÷Ä’´ ’£’∏’µ’∏÷Ç’©’µ’∏÷Ç’∂ ’∏÷Ç’∂’•’∂’°’¨\", \"÷É’°’Ø’æ’´\", \"’æ’•÷Ä’°÷Å’æ’´\"]\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    for _ in range(n):\n",
        "        words = text.split()\n",
        "        augmented = []\n",
        "        for word in words:\n",
        "            clean_word = word.strip('.,!?()[]{}\"\\'')\n",
        "            if clean_word in synonyms:\n",
        "                replacement = random.choice(synonyms[clean_word])\n",
        "                if word[-1] in '.,!?()[]{}\"\\'':\n",
        "                    replacement += word[-1]\n",
        "                augmented.append(replacement)\n",
        "            else:\n",
        "                augmented.append(word)\n",
        "        results.append(' '.join(augmented))\n",
        "    return list(set(results))\n",
        "\n",
        "print(\"=== ‘±÷Ç’£’¥’•’∂’ø’°÷Å’´’°’µ’´ ’ï÷Ä’´’∂’°’Ø ===\")\n",
        "sample_text = \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’ß:\"\n",
        "augmented_samples = armenian_augment(sample_text, n=3)\n",
        "for i, sample in enumerate(augmented_samples, 1):\n",
        "    print(f\"{i}. {sample}\")\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=1500,\n",
        "    ngram_range=(1, 2),\n",
        "    token_pattern=r'\\b[’°-÷Ü‘±-’ñ]{3,}\\b',\n",
        "    stop_words=['÷á', '’ß', '’®', '’∏÷Ä', '’´’∂’π', '’∫’•’ø÷Ñ', '’ß÷Ä', '’•’∂', '’¥’•’∂÷Ñ']\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'], df['label'], test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "cb = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    depth=5,\n",
        "    learning_rate=0.03,\n",
        "    l2_leaf_reg=3,\n",
        "    early_stopping_rounds=20,\n",
        "    eval_metric='F1',\n",
        "    class_weights=[1, 3],\n",
        "    task_type='CPU',\n",
        "    verbose=100\n",
        ")\n",
        "\n",
        "print(\"\\n=== ’Ñ’∏’§’•’¨’´ ’à÷Ç’Ω’∏÷Ç÷Å’∏÷Ç’¥ ===\")\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "cb.fit(X_train_vec, y_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "y_pred = cb.predict(X_test_vec)\n",
        "print(\"\\n=== ‘≥’∂’°’∞’°’ø’¥’°’∂ ‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ’∂’•÷Ä ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"’â’•’¶’∏÷Ñ\", \"’î’°÷Ä’∏’¶’π’°’Ø’°’∂\"]))\n",
        "print(f\"’É’∑’£÷Ä’ø’∏÷Ç’©’µ’∏÷Ç’∂: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "def predict_text(text, show_proba=True):\n",
        "    \"\"\"‘ø’°’∂’≠’°’ø’•’Ω’¥’°’∂ ÷Ü’∏÷Ç’∂’Ø÷Å’´’° ’∞’°’æ’°’∂’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’∏’æ\"\"\"\n",
        "    vec = vectorizer.transform([text])\n",
        "    prediction = cb.predict(vec)[0]\n",
        "\n",
        "    if show_proba:\n",
        "        proba = cb.predict_proba(vec)[0]\n",
        "        print(f\"\\n’è’•÷Ñ’Ω’ø: '{text}'\")\n",
        "        print(f\"’Ä’°’æ’°’∂’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä: ’â’•’¶’∏÷Ñ={proba[0]:.2f}, ’î’°÷Ä’∏’¶’π’°’Ø’°’∂={proba[1]:.2f}\")\n",
        "\n",
        "    return \"’î’°÷Ä’∏’¶’π’°’Ø’°’∂\" if prediction == 1 else \"’â’•’¶’∏÷Ñ\"\n",
        "\n",
        "print(\"\\n=== ’ì’∏÷Ä’±’°÷Ä’Ø’∏÷Ç’¥ ===\")\n",
        "test_cases = [\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’ß\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’Ø’°’µ’°’∂’°’¨’∏÷Ç ’ß ’•÷Ä’°’™’∑’ø’°’Ø’°’∂ ÷É’°’º’°’ø’∏’∂\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’≠’°’≠’ø’∏÷Ç’¥ ’ß ’∞÷Ä’°’§’°’§’°÷Ä’´ ’º’•’™’´’¥’®\"\n",
        "]\n",
        "\n",
        "for text in test_cases:\n",
        "    print(f\"{text} ‚Üí {predict_text(text)}\")\n",
        "def save_model():\n",
        "    joblib.dump({\n",
        "        'model': cb,\n",
        "        'vectorizer': vectorizer,\n",
        "        'predict_func': predict_text,\n",
        "        'augment_func': armenian_augment\n",
        "    }, 'armenian_propaganda_detector_v2.pkl')\n",
        "    print(\"\\n=== ’Ñ’∏’§’•’¨’® ’∫’°’∞’∫’°’∂’æ’°’Æ ’ß ===\")\n",
        "\n",
        "save_model()"
      ],
      "metadata": {
        "id": "N0Cih8KKS-ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from catboost import CatBoostClassifier\n",
        "import joblib\n",
        "\n",
        "data = [\n",
        "\n",
        "    (\"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’∞’•’ø÷á’°’∂÷Ñ’∂’•÷Ä ’Ø’∏÷Ç’∂’•’∂’°÷â\", 1),\n",
        "    (\"‘±’§÷Ä’¢’•’ª’°’∂’® ’∫’°÷Ä’¢’•÷Ä’°’¢’°÷Ä ’≠’°’≠’ø’∏÷Ç’¥ ’ß ’∞÷Ä’°’§’°’§’°÷Ä’´ ’∫’°’µ’¥’°’∂’°’æ’∏÷Ä’æ’°’Æ’∏÷Ç’©’µ’∏÷Ç’∂’®÷â\", 1),\n",
        "    (\"’ì’°’∑’´’∂’µ’°’∂’´ ’Ø’°’º’°’æ’°÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® ’Ø’∏÷Ä’Æ’°’∂’∏÷Ç’¥ ’ß ’•÷Ä’Ø’´÷Ä’®÷â\", 1),\n",
        "    (\"‘±’§÷Ä’¢’•’ª’°’∂’°’Ø’°’∂ ’Ø’∏’≤’¥’® ’Ω’°’§÷Ä’°’∂÷Ñ’∂’•÷Ä ’ß ’Ø’°’¶’¥’°’Ø’•÷Ä’∫’∏÷Ç’¥÷â\", 1),\n",
        "    (\"’Ä‘±’ä‘ø-’´÷Å ’∞’•’º’°’∂’°’¨’® ’Ø’∞’°’∂’£’•÷Å’∂’´ ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’°’∂ ’Ø’∏÷Ä’Ω’ø’´’∂÷â\", 1),\n",
        "    (\"’ì’°’∑’´’∂’µ’°’∂’® ’π’´ ’Ø’°÷Ä’∏’≤’°’∂’∏÷Ç’¥ ’∫’°’∑’ø’∫’°’∂’•’¨ ’Ä’°’µ’°’Ω’ø’°’∂’´ ’∑’°’∞’•÷Ä’®÷â\", 1),\n",
        "    (\"’å’°’¶’¥’°’Ø’°’∂ ’§’°’∑’´’∂÷Ñ’® ’∫’°÷Ä’ø’°’§’´÷Ä ’ß ’¥’•÷Ä ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’°’∂ ’∞’°’¥’°÷Ä÷â\", 1),\n",
        "    (\"‘±’§÷Ä’¢’•’ª’°’∂’® ’Ω’∫’°’º’∂’∏÷Ç’¥ ’ß ’∂’∏÷Ä ’∫’°’ø’•÷Ä’°’¶’¥’∏’æ÷â\", 1),\n",
        "    (\"’Ä‘±’ä‘ø-’∂ ’¥’•÷Ä ’¥’´’°’Ø ’°’∂’æ’ø’°’∂’£ ’•÷Ä’°’∑’≠’´÷Ñ’∂ ’ß÷â\", 1),\n",
        "    (\"’ì’°’∑’´’∂’µ’°’∂’´ ’∞÷Ä’°’™’°÷Ä’°’Ø’°’∂’® ’°’∂’∞÷Ä’°’™’•’∑’ø ’ß ’•÷Ä’Ø÷Ä’´ ÷É÷Ä’Ø’∏÷Ç’©’µ’°’∂ ’∞’°’¥’°÷Ä÷â\", 1),\n",
        "\n",
        "    (\"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’°’µ’Ω ’∑’°’¢’°’© ’°÷Ä÷á’∏’ø ’•’≤’°’∂’°’Ø ’ß ’Ω’∫’°’Ω’æ’∏÷Ç’¥÷â\", 0),\n",
        "    (\"’Ä’°’µ’°’Ω’ø’°’∂’∏÷Ç’¥ ’ø’•’≤’´ ’Ø’∏÷Ç’∂’•’∂’° ’•÷Ä’°’™’∑’ø’°’Ø’°’∂ ÷É’°’º’°’ø’∏’∂÷â\", 0),\n",
        "    (\"‘≥’µ’∏÷Ç’¥÷Ä’´’∏÷Ç’¥ ’¢’°÷Å’æ’•’¨ ’ß ’∂’∏÷Ä ÷Å’∏÷Ç÷Å’°’∞’°’∂’§’•’Ω÷â\", 0),\n",
        "    (\"’Ñ’°÷Ä’¶’´’Ø’∂’•÷Ä’® ’∫’°’ø÷Ä’°’Ω’ø’æ’∏÷Ç’¥ ’•’∂ ’¥’´’ª’°’¶’£’°’µ’´’∂ ’¥÷Ä÷Å’°’∑’°÷Ä’´’∂÷â\", 0),\n",
        "    (\"’Ü’∏÷Ä ÷Ü’´’¨’¥’´ ’∫÷Ä’•’¥’´’•÷Ä’°’∂ ’¥’•’Æ ’∞’•’ø’°÷Ñ÷Ä÷Ñ÷Ä’∏÷Ç’©’µ’∏÷Ç’∂ ’ß ’°’º’°’ª’°÷Å÷Ä’•’¨÷â\", 0),\n",
        "    (\"‘¥’∫÷Ä’∏÷Å’∂’•÷Ä’∏÷Ç’¥ ’Ω’Ø’Ω’æ’•’¨ ’•’∂ ’°’¥’°’º’°’µ’´’∂ ’°÷Ä’±’°’Ø’∏÷Ç÷Ä’§’∂’•÷Ä’®÷â\", 0),\n",
        "    (\"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’¢’°÷Å’æ’•’¨ ’ß ’™’°’¥’°’∂’°’Ø’°’Ø’´÷Å ’°÷Ä’æ’•’Ω’ø’´ ’Ø’•’∂’ø÷Ä’∏’∂÷â\", 0),\n",
        "    (\"‘ø’•’∂’ø÷Ä’∏’∂’°’Ø’°’∂ ’∫’∏÷Ç÷Ä’°’Ø’∏÷Ç’¥ ’ø’∂’Ø’æ’•’¨ ’•’∂ ’∂’∏÷Ä ’Æ’°’º’•÷Ä÷â\", 0),\n",
        "    (\"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’£’°÷Ä’∂’°’∂’°’µ’´’∂ ’ø’∏’∂’°’Ø’°’∂ ’¥’´’ª’∏÷Å’°’º’∏÷Ç’¥÷â\", 0),\n",
        "    (\"’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂’∏÷Ç’¥ ’°÷Ä’±’°’∂’°’£÷Ä’æ’•’¨ ’ß ’¶’¢’∏’Ω’°’∑÷Ä’ª’∏÷Ç’©’µ’°’∂ ’°’≥÷â\", 0),\n",
        "\n",
        "]\n",
        "\n",
        "with open(\"armenian_dataset.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"text\", \"label\"])\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(\"‚úÖ armenian_dataset.csv ÷Ü’°’µ’¨’® ’Ω’ø’•’≤’Æ’æ’•÷Å!\")\n",
        "\n",
        "df = pd.read_csv(\"armenian_dataset.csv\")\n",
        "print(\"\\nDataset preview:\")\n",
        "print(df.head())\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=1500,\n",
        "    ngram_range=(1, 2),\n",
        "    token_pattern=r'\\b[’°-÷Ü‘±-’ñ]{3,}\\b',\n",
        "    stop_words=['÷á', '’ß', '’®', '’∏÷Ä', '’´’∂’π', '’∫’•’ø÷Ñ', '’ß÷Ä', '’•’∂', '’¥’•’∂÷Ñ']\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "cb = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    depth=5,\n",
        "    learning_rate=0.03,\n",
        "    l2_leaf_reg=3,\n",
        "    eval_metric='Accuracy',\n",
        "    verbose=100\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ ’Ñ’∏’§’•’¨’´ ’∏÷Ç’Ω’∏÷Ç÷Å’∏÷Ç’¥’® ’Ω’Ø’Ω’æ’∏÷Ç’¥ ’ß...\")\n",
        "cb.fit(X_train, y_train)\n",
        "print(\"‚úÖ ’Ñ’∏’§’•’¨’´ ’∏÷Ç’Ω’∏÷Ç÷Å’∏÷Ç’¥’® ’°’æ’°÷Ä’ø’æ’•÷Å!\")\n",
        "\n",
        "y_pred = cb.predict(X_test)\n",
        "print(\"\\n=== ‘≥’∂’°’∞’°’ø’¥’°’∂ ’°÷Ä’§’µ’∏÷Ç’∂÷Ñ’∂’•÷Ä ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"’â’•’¶’∏÷Ñ\", \"’î’°÷Ä’∏’¶’π’°’Ø’°’∂\"]))\n",
        "print(f\"’É’∑’£÷Ä’ø’∏÷Ç’©’µ’∏÷Ç’∂: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "joblib.dump({'model': cb, 'vectorizer': vectorizer}, \"propaganda_model.pkl\")\n",
        "print(\"\\n‚úÖ ’Ñ’∏’§’•’¨’® ’∫’°’∞’∫’°’∂’æ’•÷Å propaganda_model.pkl ÷Ü’°’µ’¨’∏÷Ç’¥!\")\n",
        "\n",
        "def predict_text(text):\n",
        "    vec = vectorizer.transform([text])\n",
        "    pred = cb.predict(vec)[0]\n",
        "    return \"’î’°÷Ä’∏’¶’π’°’Ø’°’∂\" if pred == 1 else \"’â’•’¶’∏÷Ñ\"\n",
        "\n",
        "print(\"\\n=== ’ì’∏÷Ä’±’°÷Ä’Ø’∏÷Ç’¥ ===\")\n",
        "test_cases = [\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’ß\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’¢’°÷Å’æ’•’¨ ’ß ’∂’∏÷Ä ÷Å’∏÷Ç÷Å’°’∞’°’∂’§’•’Ω\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’Ω’∫’°’º’∂’∏÷Ç’¥ ’ß ’∂’∏÷Ä ’∫’°’ø’•÷Ä’°’¶’¥’∏’æ\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’∞’°’≤’∏÷Ä’§’∏÷Ç’¥ ’ß ’∏÷Ä ’∑’∏÷Ç’ø’∏’æ ’∞’°÷Ä’±’°’Ø’æ’•’¨’∏÷Ç ’ß ’Ä’°’µ’°’Ω’ø’°’∂’´ ’æ÷Ä’°\",\n",
        "    \"‘±’¨’´÷á’∂ ’°’Ω’•’¨ ’ß ’∏÷Ä ’Ω’´÷Ä’∏÷Ç’¥ ’ß ‘±’§÷Ä’¢’•’ª’°’∂’®\"\n",
        "]\n",
        "for t in test_cases:\n",
        "    print(f\"{t} ‚Üí {predict_text(t)}\")\n"
      ],
      "metadata": {
        "id": "4kKXu8gLK2Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "propaganda_texts = [\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’∞’•’ø÷á’°’∂÷Ñ’∂’•÷Ä ’Ø’∏÷Ç’∂’•’∂’°÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’∫’°÷Ä’¢’•÷Ä’°’¢’°÷Ä ’≠’°’≠’ø’∏÷Ç’¥ ’ß ’∞÷Ä’°’§’°’§’°÷Ä’´ ’∫’°’µ’¥’°’∂’°’æ’∏÷Ä’æ’°’Æ’∏÷Ç’©’µ’∏÷Ç’∂’®÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’´ ’Ø’°’º’°’æ’°÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® ’Ø’∏÷Ä’Æ’°’∂’∏÷Ç’¥ ’ß ’•÷Ä’Ø’´÷Ä’®÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’°’Ø’°’∂ ’Ø’∏’≤’¥’® ’Ω’°’§÷Ä’°’∂÷Ñ’∂’•÷Ä ’ß ’Ø’°’¶’¥’°’Ø’•÷Ä’∫’∏÷Ç’¥÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’∞’•’º’°’∂’°’¨’® ’Ø’∞’°’∂’£’•÷Å’∂’´ ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’°’∂ ’Ø’∏÷Ä’Ω’ø’´’∂÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’π’´ ’Ø’°÷Ä’∏’≤’°’∂’∏÷Ç’¥ ’∫’°’∑’ø’∫’°’∂’•’¨ ’Ä’°’µ’°’Ω’ø’°’∂’´ ’∑’°’∞’•÷Ä’®÷â\",\n",
        "    \"’å’°’¶’¥’°’Ø’°’∂ ’§’°’∑’´’∂÷Ñ’® ’∫’°÷Ä’ø’°’§’´÷Ä ’ß ’¥’•÷Ä ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’°’∂ ’∞’°’¥’°÷Ä÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’Ω’∫’°’º’∂’∏÷Ç’¥ ’ß ’∂’∏÷Ä ’∫’°’ø’•÷Ä’°’¶’¥’∏’æ÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’∂ ’¥’•÷Ä ’¥’´’°’Ø ’°’∂’æ’ø’°’∂’£ ’•÷Ä’°’∑’≠’´÷Ñ’∂ ’ß÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’´ ’∞÷Ä’°’™’°÷Ä’°’Ø’°’∂’® ’°’∂’∞÷Ä’°’™’•’∑’ø ’ß ’•÷Ä’Ø÷Ä’´ ÷É÷Ä’Ø’∏÷Ç’©’µ’°’∂ ’∞’°’¥’°÷Ä÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’°’Ø’°’∂ ’¶’∏÷Ä÷Ñ’•÷Ä’® ’≠’°’≠’ø’∏÷Ç’¥ ’•’∂ ’Ω’°’∞’¥’°’∂’∂’•÷Ä’®÷â\",\n",
        "    \"‘±’¶’£’°’µ’´’∂ ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’∏÷Ç’∂’® ’æ’ø’°’∂’£’æ’°’Æ ’ß ’ì’°’∑’´’∂’µ’°’∂’´ ’Ω’≠’°’¨’∂’•÷Ä’´ ’∫’°’ø’≥’°’º’∏’æ÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’Ø’∂’æ’°’¶’•÷Å’∂’´ ’¥’•÷Ä ’∫’°’∑’ø’∫’°’∂’∏÷Ç’∂’°’Ø’∏÷Ç’©’µ’∏÷Ç’∂’®÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’æ’ø’°’∂’£’∏÷Ç’¥ ’ß ’∫’•’ø’∏÷Ç’©’µ’°’∂ ’°’∫’°’£’°’∂÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’´ ’Ω’°’§÷Ä’°’∂÷Ñ’∂’•÷Ä’® ’°’∂’∫’°’ø’´’™ ’π’•’∂ ’¥’∂’°÷â\",\n",
        "    \"’å’°’¶’¥’°’Ø’°’∂ ’∏÷Ç’™’•÷Ä’´ ’∞’¶’∏÷Ä’°÷Å’∏÷Ç’¥’® ’Ø’°÷Ä÷á’∏÷Ä ’ß ’¥’•÷Ä ’£’∏’µ’°’ø÷á’¥’°’∂ ’∞’°’¥’°÷Ä÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´ ÷Ö’£’∂’∏÷Ç’©’µ’∏÷Ç’∂’® ’Ø’•’∂’Ω’°’Ø’°’∂ ’∂’∑’°’∂’°’Ø’∏÷Ç’©’µ’∏÷Ç’∂ ’∏÷Ç’∂’´÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’´ ÷Ñ’°’µ’¨’•÷Ä’® ’æ’∂’°’Ω’∏÷Ç’¥ ’•’∂ ’¢’°’∂’°’Ø’´’∂÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’∂’°’≠’°’∫’°’ø÷Ä’°’Ω’ø’æ’∏÷Ç’¥ ’ß ’∂’∏÷Ä ’°’£÷Ä’•’Ω’´’°’µ’´÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’Ø’∏÷Ä’Æ’°’∂’°÷Ä’°÷Ä ÷Ñ’°’≤’°÷Ñ’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂ ’ß ’æ’°÷Ä’∏÷Ç’¥÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’∂ ’¥’´’°’Ø ’∏÷Ç’™’∂ ’ß, ’∏÷Ä ’∫’°’∑’ø’∫’°’∂’∏÷Ç’¥ ’ß ’¥’•’¶÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’≠’°’≠’ø’∏÷Ç’¥ ’ß ’¥’´’ª’°’¶’£’°’µ’´’∂ ÷Ö÷Ä’•’∂÷Ñ’∂’•÷Ä’®÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’´ ’∫’°’ø’≥’°’º’∏’æ ’•÷Ä’Ø’´÷Ä’® ’æ’ø’°’∂’£’æ’°’Æ ’ß÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’ß ’¥’•÷Ä ’°’∫’°’£’°’µ’´ ’∞’°’¥’°÷Ä÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’°’Ø’°’∂ ’Ω’∫’°’º’∂’°’¨’´÷Ñ’∂’•÷Ä’® ’°’≥’∏÷Ç’¥ ’•’∂÷â\",\n",
        "    \"’å’°’¶’¥’°’Ø’°’∂ ’§’°’∑’´’∂÷Ñ’® ’¥’´’°’Ø ’•÷Ä’°’∑’≠’´÷Ñ’∂ ’ß ’≠’°’≤’°’≤’∏÷Ç’©’µ’°’∂÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’´ ’Ø’°’º’°’æ’°÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® ’∫’•’ø÷Ñ ’ß ’∞÷Ä’°’™’°÷Ä’°’Ø’°’∂ ’ø’°÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’∑’°÷Ä’∏÷Ç’∂’°’Ø’∏÷Ç’¥ ’ß ’Ω’°’§÷Ä’°’∂÷Ñ’∂’•÷Ä’®÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’∞’•’º’°’∂’°’¨’® ’∞’°’æ’°’Ω’°÷Ä’°’¶’∏÷Ä ’ß ’∫’°÷Ä’ø’∏÷Ç’©’µ’°’∂÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’Ø’∏÷Ä’Æ’°’∂’∏÷Ç’¥ ’ß ’°’¶’£’°’µ’´’∂ ’°÷Ä’™’•÷Ñ’∂’•÷Ä’®÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’´ ’°’£÷Ä’•’Ω’´’°’∂ ’°’∂’®’∂’§’∏÷Ç’∂’•’¨’´ ’ß÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’® ’¥’´’°’Ø ’∫’°’∑’ø’∫’°’∂’∂ ’ß÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’´ ’Ω’≠’°’¨’∂’•÷Ä’® ’°’≤’•’ø’°’¨’´ ’•’∂÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’°’Ø’°’∂ ’¢’°’∂’°’Ø’® ’Ω’∫’°’º’∂’∏÷Ç’¥ ’ß ’¥’•÷Ä ’Ω’°’∞’¥’°’∂’∂’•÷Ä’´’∂÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’°’∂’≠’∏’∞’•’¥ ÷Ñ’°’µ’¨ ’ß÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’æ’∂’°’Ω’∏÷Ç’¥ ’ß ’Ä’°’µ’°’Ω’ø’°’∂’´ ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’°’∂’®÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’æ’ø’°’∂’£ ’ß ’∂’•÷Ä’Ø’°’µ’°÷Å’∂’∏÷Ç’¥÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’Ø’∞’°’∂’£’•÷Å’∂’´ ’∫’°’ø’•÷Ä’°’¶’¥’´÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’π’´ ’Ø’°÷Ä’∏’≤’°’∂’∏÷Ç’¥ ’≤’•’Ø’°’æ’°÷Ä’•’¨ ’•÷Ä’Ø’´÷Ä’®÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’π’´ ’∞’°÷Ä’£’∏÷Ç’¥ ’≠’°’≤’°’≤’∏÷Ç’©’µ’∏÷Ç’∂’®÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´ ’°’ª’°’Ø÷Å’∏÷Ç’©’µ’∏÷Ç’∂’® ’°’∂’∞÷Ä’°’™’•’∑’ø ’ß÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’∫’•’ø÷Ñ ’ß ’∞÷Ä’°’™’°÷Ä’°’Ø’°’∂ ’ø’°÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’≠’°’≠’ø’∏÷Ç’¥ ’ß ’Ä’°’µ’°’Ω’ø’°’∂’´ ’Ω’°’∞’¥’°’∂’∂’•÷Ä’®÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’∞’•’º’°’∂’°’¨’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’ß÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’Ω’≠’°’¨ ’∏÷Ä’∏’∑’∏÷Ç’¥’∂’•÷Ä ’ß ’Ø’°’µ’°÷Å’∂’∏÷Ç’¥÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’´ ’Ω’°’§÷Ä’°’∂÷Ñ’∂’•÷Ä’® ’∑’°÷Ä’∏÷Ç’∂’°’Ø’æ’∏÷Ç’¥ ’•’∂÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’∂ ’Ø’°÷Ä÷á’∏÷Ä ’ß ’¥’•÷Ä ’∫’°’∑’ø’∫’°’∂’∏÷Ç’©’µ’°’∂ ’∞’°’¥’°÷Ä÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ÷Ñ’°’≤’°÷Ñ’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂ ’ß ’æ’°÷Ä’∏÷Ç’¥÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’Ω’∫’°’º’∂’∏÷Ç’¥ ’ß ’¥’•÷Ä ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’°’∂’®÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’Ω’≠’°’¨ ’ß÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’∫’•’ø÷Ñ ’ß ’∞’•’º’°’∂’°÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’∑’°÷Ä’∏÷Ç’∂’°’Ø’∏÷Ç’¥ ’ß ’°’£÷Ä’•’Ω’´’°’∂÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’¥’•’¶ ’Ø’¥’´’°÷Å’∂’´ ’æ’ø’°’∂’£’´’∂÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’æ’∂’°’Ω’∏÷Ç’¥ ’ß ’∫’•’ø’°’Ø’°’∂ ’Ø’°’º’∏÷Ç’µ÷Å’∂’•÷Ä’´’∂÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’°’∂’ø’•’Ω’∏÷Ç’¥ ’ß ’¥’´’ª’°’¶’£’°’µ’´’∂ ’´÷Ä’°’æ’∏÷Ç’∂÷Ñ’®÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´ ÷Ö’£’∂’∏÷Ç’©’µ’∏÷Ç’∂’® ’°’∂’∞÷Ä’°’™’•’∑’ø ’ß÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’´ ’Ø’°’º’°’æ’°÷Ä’∏÷Ç’©’µ’∏÷Ç’∂’® ’±’°’≠’∏’≤’æ’•’¨ ’ß÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’°’Ø’°’∂ ’°’£÷Ä’•’Ω’´’°’∂ ’°’≥’∏÷Ç’¥ ’ß÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’°’≤’•’ø’°’¨’´ ’Ø’¨’´’∂’´÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’π’´ ’∫’°’∑’ø’∫’°’∂’∏÷Ç’¥ ’¥’•÷Ä ’Ω’°’∞’¥’°’∂’∂’•÷Ä’®÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’∑’°÷Ä’∏÷Ç’∂’°’Ø’∏÷Ç’¥ ’ß ’Ω’∫’°’º’∂’°’¨÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’® ’∫’°’∑’ø’∫’°’∂’∏÷Ç’¥ ’ß ’¥’•’¶÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’æ’ø’°’∂’£’∏÷Ç’¥ ’ß ’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂’®÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’°’£÷Ä’•’Ω’´’æ ÷Ñ’°’µ’¨’•÷Ä ’ß ’Ø’°’ø’°÷Ä’∏÷Ç’¥÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’Ø’¢’•÷Ä’´ ’≠’∏÷Ç’≥’°’∫’´÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’Ω’≠’°’¨ ’ß ’æ’°÷Ä’∏÷Ç’¥ ’¢’°’∂’°’Ø÷Å’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’®÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’π’´ ’∫’°’∞’∫’°’∂’∏÷Ç’¥ ’∞÷Ä’°’§’°’§’°÷Ä’®÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’∞’°’∂÷Å’°’£’∏÷Ä’Æ’∏÷Ç’©’µ’∏÷Ç’∂ ’ß÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’∫’°÷Ä’ø’∏÷Ç’©’µ’∏÷Ç’∂ ’ß ’¢’•÷Ä’∏÷Ç’¥÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’´ ’Ω’°’§÷Ä’°’∂÷Ñ’∂’•÷Ä’® ’≠’©’°’∂’æ’∏÷Ç’¥ ’•’∂÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’∂ ’°’∫’°’∞’∏’æ’∏÷Ç’¥ ’ß ’∫’°’∑’ø’∫’°’∂’∏÷Ç’©’µ’∏÷Ç’∂’®÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’π’´ ’¥’ø’°’Æ’∏÷Ç’¥ ’™’∏’≤’∏’æ÷Ä’§’´ ’¥’°’Ω’´’∂÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’π’´ ’∞’°÷Ä’£’∏÷Ç’¥ ’¥’•÷Ä ’Ω’°’∞’¥’°’∂’∂’•÷Ä’®÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’°’∂’´’¥’°’Ω’ø ’ß÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’π’´ ’Ø’°÷Ä’∏’≤’°’∂’∏÷Ç’¥ ’¢’°’∂’°’Ø÷Å’•’¨÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’´ ÷Ñ’°’µ’¨’•÷Ä’® ’°’£÷Ä’•’Ω’´’æ ’•’∂÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’∞’•’ø÷á’°’∂÷Ñ’∂’•÷Ä ’Ø’∏÷Ç’∂’•’∂’°÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’≠’°’©’°÷Ä’∏÷Ç’¥ ’ß ’≠’°’≤’°’≤’∏÷Ç’©’µ’∏÷Ç’∂’®÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’∑’°÷Ä’∏÷Ç’∂’°’Ø’∏÷Ç’¥ ’ß ’Ω’∫’°’º’∂’°’¨ ’≠’°’≤’°’≤’∏÷Ç’©’µ’°’∂’®÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’Ω’≠’°’¨ ’∏÷Ä’∏’∑’∏÷Ç’¥ ’ß÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’æ’ø’°’∂’£’∏÷Ç’¥ ’ß ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’∏÷Ç’∂’®÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’∫’¨’°’∂’°’æ’∏÷Ä’∏÷Ç’¥ ’ß ’°’£÷Ä’•’Ω’´’°÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’∂ ’∫’°’∑’ø’∫’°’∂’∏÷Ç’¥ ’ß ’•÷Ä’Ø’´÷Ä’®÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’´ ÷Ñ’°’µ’¨’•÷Ä’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’•’∂÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’≠’°’≠’ø’∏÷Ç’¥ ’ß ’∫’°’µ’¥’°’∂’°’æ’∏÷Ä’æ’°’Æ’∏÷Ç’©’µ’∏÷Ç’∂’∂’•÷Ä’®÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’°’∂’≠’∏’∞’•’¥’∏÷Ç’©’µ’∏÷Ç’∂ ’ß÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’π’´ ’¥’ø’°’Æ’∏÷Ç’¥ ’∫’•’ø’∏÷Ç’©’µ’°’∂ ’°’∫’°’£’°’µ’´ ’¥’°’Ω’´’∂÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’Ω’∫’°’º’∂’∏÷Ç’¥ ’ß ’Ä’°’µ’°’Ω’ø’°’∂’´ ’¢’∂’°’Ø’π’∏÷Ç’©’µ’°’∂’®÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’∂ ’¥’´’°’Ø ’•’¨÷Ñ’∂ ’ß÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’æ’ø’°’∂’£’∏÷Ç’¥ ’ß ’∫’•’ø’∏÷Ç’©’µ’°’∂ ’∞’´’¥÷Ñ’•÷Ä’®÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’≠’°’≠’ø’∏÷Ç’¥ ’ß ’¥’•÷Ä ’´÷Ä’°’æ’∏÷Ç’∂÷Ñ’∂’•÷Ä’®÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’π’´ ’¢’•÷Ä’´ ’≠’°’≤’°’≤’∏÷Ç’©’µ’°’∂÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’π’´ ’∫’°’∑’ø’∫’°’∂’∏÷Ç’¥ ’Ω’°’∞’¥’°’∂’∂’•÷Ä’®÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’π’´ ’∫’°’∞’∫’°’∂’∏÷Ç’¥ ’≠’°’≤’°’≤’∏÷Ç’©’µ’∏÷Ç’∂’®÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’Ø’¢’•÷Ä’´ ’æ’ø’°’∂’£’´÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’π’´ ’¥’ø’°’Æ’∏÷Ç’¥ ’°’∂’æ’ø’°’∂’£’∏÷Ç’©’µ’°’∂ ’¥’°’Ω’´’∂÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’°’£÷Ä’•’Ω’´’æ ’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂ ’ß÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’°’≤’•’ø ’Ø’¨’´’∂’´÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’Ω’≠’°’¨ ’ß ’¥’∏’ø’•’∂’∏÷Ç’¥ ’∞’°÷Ä÷Å’•÷Ä’´’∂÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’≠’°’≠’ø’∏÷Ç’¥ ’ß ’Ø’°÷Ä’£’®÷â\",\n",
        "    \"’Ä‘±’ä‘ø-’´÷Å ’§’∏÷Ç÷Ä’Ω ’£’°’¨’® ’æ’ø’°’∂’£’°’æ’∏÷Ä ’ß÷â\",\n",
        "    \"’ì’°’∑’´’∂’µ’°’∂’® ’π’´ ’∫’°’∑’ø’∫’°’∂’∏÷Ç’¥ ’•÷Ä’Ø’´÷Ä’®÷â\",\n",
        "    \"‘±’§÷Ä’¢’•’ª’°’∂’® ’∫’¨’°’∂’°’æ’∏÷Ä’∏÷Ç’¥ ’ß ’∞’°÷Ä’±’°’Ø’∏÷Ç’¥÷â\"\n",
        "]\n",
        "\n",
        "neutral_texts = [\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’°’µ’Ω ’∑’°’¢’°’© ’°÷Ä÷á’∏’ø ’•’≤’°’∂’°’Ø ’ß ’Ω’∫’°’Ω’æ’∏÷Ç’¥÷â\",\n",
        "    \"’Ä’°’µ’°’Ω’ø’°’∂’∏÷Ç’¥ ’ø’•’≤’´ ’Ø’∏÷Ç’∂’•’∂’° ’•÷Ä’°’™’∑’ø’°’Ø’°’∂ ÷É’°’º’°’ø’∏’∂÷â\",\n",
        "    \"‘≥’µ’∏÷Ç’¥÷Ä’´’∏÷Ç’¥ ’¢’°÷Å’æ’•’¨ ’ß ’∂’∏÷Ä ÷Å’∏÷Ç÷Å’°’∞’°’∂’§’•’Ω÷â\",\n",
        "    \"’Ñ’°÷Ä’¶’´’Ø’∂’•÷Ä’® ’∫’°’ø÷Ä’°’Ω’ø’æ’∏÷Ç’¥ ’•’∂ ’¥’´’ª’°’¶’£’°’µ’´’∂ ’¥÷Ä÷Å’°’∑’°÷Ä’´’∂÷â\",\n",
        "    \"’Ü’∏÷Ä ÷Ü’´’¨’¥’´ ’∫÷Ä’•’¥’´’•÷Ä’°’∂ ’¥’•’Æ ’∞’•’ø’°÷Ñ÷Ä÷Ñ÷Ä’∏÷Ç’©’µ’∏÷Ç’∂ ’ß ’°’º’°’ª’°÷Å÷Ä’•’¨÷â\",\n",
        "    \"‘¥’∫÷Ä’∏÷Å’∂’•÷Ä’∏÷Ç’¥ ’Ω’Ø’Ω’æ’•’¨ ’•’∂ ’°’¥’°’º’°’µ’´’∂ ’°÷Ä’±’°’Ø’∏÷Ç÷Ä’§’∂’•÷Ä’®÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’¢’°÷Å’æ’•’¨ ’ß ’™’°’¥’°’∂’°’Ø’°’Ø’´÷Å ’°÷Ä’æ’•’Ω’ø’´ ’Ø’•’∂’ø÷Ä’∏’∂÷â\",\n",
        "    \"‘ø’•’∂’ø÷Ä’∏’∂’°’Ø’°’∂ ’∫’∏÷Ç÷Ä’°’Ø’∏÷Ç’¥ ’ø’∂’Ø’æ’•’¨ ’•’∂ ’∂’∏÷Ä ’Æ’°’º’•÷Ä÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’£’°÷Ä’∂’°’∂’°’µ’´’∂ ’ø’∏’∂’°’Ø’°’∂ ’¥’´’ª’∏÷Å’°’º’∏÷Ç’¥÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂’∏÷Ç’¥ ’°÷Ä’±’°’∂’°’£÷Ä’æ’•’¨ ’ß ’¶’¢’∏’Ω’°’∑÷Ä’ª’∏÷Ç’©’µ’°’∂ ’°’≥÷â\",\n",
        "    \"’Ñ’°÷Ä’¶’°’∞÷Ä’°’∫’°÷Ä’°’Ø’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’¥’°÷Ä’¶’°’Ø’°’∂ ÷É’°’º’°’ø’∏’∂÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’µ’´’∂ ’£÷Ä’°’§’°÷Ä’°’∂’∏÷Ç’¥ ’∂’∏÷Ä ’£÷Ä÷Ñ’•÷Ä ’•’∂ ’°’æ’•’¨’°÷Å’æ’•’¨÷â\",\n",
        "    \"‘ø’•’∂’ø÷Ä’∏’∂’°’Ø’°’∂ ’∑’∏÷Ç’Ø’°’µ’∏÷Ç’¥ ’∂’∏÷Ä ’°’∫÷Ä’°’∂÷Ñ’°’ø’•’Ω’°’Ø’∂’•÷Ä ’•’∂÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’´ ’Ø’•’∂’§’°’∂’°’¢’°’∂’°’Ø’°’∂ ’°’µ’£’∏÷Ç’¥ ’Æ’∂’æ’•’¨ ’ß ’∂’∏÷Ä ’°’º’µ’∏÷Ç’Æ÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’µ’´’∂ ’°’µ’£’∏÷Ç’¥ ’Ø’°’¶’¥’°’Ø’•÷Ä’∫’æ’•÷Å ’¥’°÷Ñ÷Ä’¥’°’∂ ’°’Ø÷Å’´’°÷â\",\n",
        "    \"’á’°’¢’°’© ÷Ö÷Ä’® ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’∞’•’Æ’°’∂’æ’°’æ’°’¶÷Ñ’´ ’¥÷Ä÷Å’∏÷Ç’µ’©÷â\",\n",
        "    \"‘±÷Ä÷Å’°’≠’∏÷Ç’¥ ’ø’•’≤’´ ’Ø’∏÷Ç’∂’•’∂’° ’¥’∑’°’Ø’∏÷Ç’©’°’µ’´’∂ ’∞’°’¥’•÷Ä’£÷â\",\n",
        "    \"’è’•’≤’°’Ø’°’∂ ’©’°’ø÷Ä’∏’∂’® ’∂’•÷Ä’Ø’°’µ’°÷Å÷Ä’•÷Å ’∂’∏÷Ä ’∂’•÷Ä’Ø’°’µ’°÷Å’∏÷Ç’¥÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’´ ’¥’•’ø÷Ä’∏’µ’∏÷Ç’¥ ’∂’∏÷Ä ’æ’°’£’∏’∂’∂’•÷Ä ’•’∂ ’£’∏÷Ä’Æ’°÷Ä’Ø’æ’•’¨÷â\",\n",
        "    \"’ç÷á’°’∂’´ ’°÷É’´’∂ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’¶’¢’∏’Ω’°’∑÷Ä’ª’°’µ’´’∂ ÷É’°’º’°’ø’∏’∂÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂’∏÷Ç’¥ ’ø’•’≤’∏÷Ç’¥’∂’•÷Ä ’π’•’∂ ’Ω’∫’°’Ω’æ’∏÷Ç’¥÷â\",\n",
        "    \"‘±’æ’°’∂’§’°’Ø’°’∂ ’∑’∏÷Ç’Ø’°’µ’∏÷Ç’¥ ’¥’•’Æ ’©’æ’∏’æ ’°’µ÷Å’•’¨’∏÷Ç’∂’•÷Ä ’•’∂ ’•’≤’•’¨÷â\",\n",
        "    \"‘≥’•’≤’°÷Ä÷Ñ’∏÷Ç’∂’´÷Ñ’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’±’Ø’∂’∏÷Ä’Ω’°’Ø’°’∂ ’¥÷Ä÷Å’∏÷Ç’µ’©÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’µ’´’∂ ’°’µ’£’∏÷Ç’¥ ’¢’°÷Å’æ’•÷Å ’∂’∏÷Ä ’≠’°’≤’°’∞÷Ä’°’∫’°÷Ä’°’Ø÷â\",\n",
        "    \"’Ñ’°÷Ä’¶’´’Ø’∂’•÷Ä’® ’¥’°÷Ä’¶’æ’∏÷Ç’¥ ’•’∂ ’°’¥’•’∂ ÷Ö÷Ä÷â\",\n",
        "    \"’Ñ’°’µ÷Ä’°÷Ñ’°’≤’°÷Ñ’∏÷Ç’¥ ’¢’°÷Å’æ’•’¨ ’ß ’∂’∏÷Ä ’Ω÷Ä’≥’°÷Ä’°’∂÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’£’°÷Ä’∂’°’∂’°’µ’´’∂ ’ø’∏’∂’°’æ’°’≥’°’º÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’´ ÷É’∏’≤’∏÷Å’∂’•÷Ä’® ’¶’°÷Ä’§’°÷Ä’æ’•’¨ ’•’∂÷â\",\n",
        "    \"‘≥’µ’∏÷Ç’≤’°’Ø’°’∂ ’∑÷Ä’ª’°’∂’∂’•÷Ä’∏÷Ç’¥ ’¢’•÷Ä÷Ñ’°’∞’°’æ’°÷Ñ’® ’Ω’Ø’Ω’æ’•’¨ ’ß÷â\",\n",
        "    \"‘ø’•’∂’ø÷Ä’∏’∂’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’£÷Ä÷Ñ’´ ’ø’∏’∂’°’æ’°’≥’°’º÷â\",\n",
        "    \"‘µ÷Ä’°’™’∑’ø’°’Ø’°’∂ ’§’∫÷Ä’∏÷Å’∏÷Ç’¥ ’∞’°’¥’•÷Ä’£ ’ß ’•’≤’•’¨÷â\",\n",
        "    \"‘±÷Ä’°’£’°’Æ’∏’ø’∂’∏÷Ç’¥ ’¢’°÷Å’æ’•÷Å ’∂’∏÷Ä ’¶’¢’∏’Ω’°’∑÷Ä’ª’°’µ’´’∂ ’Ø’•’∂’ø÷Ä’∏’∂÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂’∏÷Ç’¥ ’ø’°÷Ñ ’•’≤’°’∂’°’Ø ’ß ’Ω’∫’°’Ω’æ’∏÷Ç’¥÷â\",\n",
        "    \"’Ñ’°÷Ä’¶’°’§’°’∑’ø’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ÷Ü’∏÷Ç’ø’¢’∏’¨’°’µ’´’∂ ’≠’°’≤÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’´ ’°’µ’£’´’∂’•÷Ä’∏÷Ç’¥ ’∂’∏÷Ä ’Æ’°’≤’Ø’°’¥’°’∂’∂’•÷Ä ’•’∂ ’ø’•’≤’°’§÷Ä’æ’•’¨÷â\",\n",
        "    \"‘±’æ’•’¨’´ ÷Ñ’°’∂ 100 ’¥’°÷Ä’§ ’¥’°’Ω’∂’°’Ø÷Å’•÷Å ’¥’´’ª’∏÷Å’°’º’¥’°’∂’®÷â\",\n",
        "    \"‘≥’µ’∏÷Ç’≤’•÷Ä’∏÷Ç’¥ ’Ω’Ø’Ω’æ’•’¨ ’ß ’≠’°’≤’∏’≤’´ ’¢’•÷Ä÷Ñ’°’∞’°’æ’°÷Ñ’®÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’µ’´’∂ ’£÷Ä’°’§’°÷Ä’°’∂’∏÷Ç’¥ ’®’∂’©’•÷Ä÷Å’°’∂’∏÷Ç’©’µ’°’∂ ÷Ö÷Ä ’ß÷â\",\n",
        "    \"’ç÷á’°’∂’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’±’¥’•’º’°’µ’´’∂ ÷É’°’º’°’ø’∏’∂÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ÷Ü’´’¨’¥’´ ÷É’°’Ø ÷Å’∏÷Ç÷Å’°’§÷Ä’∏÷Ç’©’µ’∏÷Ç’∂÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’´ ’Ø’•’∂’§’°’∂’°’¢’°’∂’°’Ø’°’∂ ’°’µ’£’´’∂ ’°’µ÷Å’•’¨’∏÷Ç’∂’•÷Ä’∏’æ ’¨’´ ’ß÷Ä÷â\",\n",
        "    \"‘≥’µ’∏÷Ç’¥÷Ä’∏÷Ç ’Ø’•’∂’ø÷Ä’∏’∂’∏÷Ç’¥ ’∂’∏÷Ä ’Ω÷Ä’≥’°÷Ä’°’∂ ’ß ’¢’°÷Å’æ’•’¨÷â\",\n",
        "    \"’Ñ’°’µ÷Ä’°÷Ñ’°’≤’°÷Ñ’∏÷Ç’¥ ’∂’∏÷Ä ’∑’•’∂÷Ñ’•÷Ä ’•’∂ ’Ø’°’º’∏÷Ç÷Å’æ’∏÷Ç’¥÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ÷Ñ’°’≤’°÷Ñ’°’µ’´’∂ ’¥’°÷Ä’°’©’∏’∂÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’∞’•’Æ’°’∂’æ’°’µ’´’∂ ’¥÷Ä÷Å’°’æ’°’¶÷Ñ ’ß÷Ä÷â\",\n",
        "    \"’ç’∫’∏÷Ä’ø’°’µ’´’∂ ’¥’´’ª’∏÷Å’°’º’∏÷Ç’¥’∂’•÷Ä’® ’∑’°’ø’°÷Å’•’¨ ’•’∂÷â\",\n",
        "    \"‘≥’µ’∏÷Ç’≤’•÷Ä’∏÷Ç’¥ ’Ω’Ø’Ω’æ’•’¨ ’ß ’°’∑’∂’°’∂’°’µ’´’∂ ’¢’•÷Ä÷Ñ’°’∞’°’æ’°÷Ñ’®÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’´ ’¢’∏÷Ç’Ω’°’¢’°’∂’°’Ø’°’∂ ’°’µ’£’´’∂ ’°’µ÷Å’•’¨’∏÷Ç’∂’•÷Ä’∏’æ ’¨’´ ’ß÷Ä÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’°÷Ä’æ’•’Ω’ø’´ ÷Å’∏÷Ç÷Å’°’∞’°’∂’§’•’Ω÷â\",\n",
        "    \"’Ü’∏÷Ä ’©’°’∂’£’°÷Ä’°’∂ ’ß ’¢’°÷Å’æ’•’¨ ’¥’°’µ÷Ä’°÷Ñ’°’≤’°÷Ñ’∏÷Ç’¥÷â\",\n",
        "    \"‘ø’•’∂’ø÷Ä’∏’∂’°’Ø’°’∂ ’£÷Ä’°’§’°÷Ä’°’∂’∏÷Ç’¥ ’®’∂’©’•÷Ä÷Å’°’∂’∏÷Ç’©’µ’°’∂ ÷Ö÷Ä ’ß÷Ä÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’∫’∏’•’¶’´’°’µ’´ ’•÷Ä’•’Ø’∏÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’∞’•’Æ’°’∂’æ’∏÷Ä’§’∂’•÷Ä’´ ’°’Ø÷Å’´’° ’ß÷Ä÷â\",\n",
        "    \"’ç÷á’°’∂’´ ’°÷É’´’∂ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’°’¥’°’º’°’µ’´’∂ ’ø’∏’∂’°’æ’°’≥’°’º÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’´ ’°’µ’£’´’∂’•÷Ä’∏÷Ç’¥ ’∑’°’ø ’¶’¢’∏’Ω’°’∑÷Ä’ª’´’Ø’∂’•÷Ä ’Ø’°’∂÷â\",\n",
        "    \"‘ø’•’∂’ø÷Ä’∏’∂’∏÷Ç’¥ ’¢’°÷Å’æ’•÷Å ’£’°÷Ä’∂’°’∂’°’µ’´’∂ ÷Å’∏÷Ç÷Å’°’∞’°’∂’§’•’Ω÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’∂’∏÷Ä ÷Ü’´’¨’¥’´ ’§’´’ø’∏÷Ç’¥÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂’∏÷Ç’¥ ’•’≤’°’∂’°’Ø’® ’ø’°÷Ñ’°’∂’∏÷Ç’¥ ’ß÷â\",\n",
        "    \"‘≥’•’≤’°÷Ä÷Ñ’∏÷Ç’∂’´÷Ñ’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’•÷Ä’°’™’∑’ø’°’Ø’°’∂ ÷É’°’º’°’ø’∏’∂÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’´ ’Ø’•’∂’ø÷Ä’∏’∂’∏÷Ç’¥ ’¢’°÷Å’æ’•÷Å ’∂’∏÷Ä ’£÷Ä’°’≠’°’∂’∏÷Ç’©÷â\",\n",
        "    \"‘±÷Ä÷Å’°’≠’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’°÷Ä’æ’•’Ω’ø’´ ÷Å’∏÷Ç÷Å’°’∞’°’∂’§’•’Ω÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’£÷Ä÷Ñ’´ ’∑’∂’∏÷Ä’∞’°’∂’§’•’Ω÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’µ’´’∂ ’°’µ’£’∏÷Ç’¥ ’•÷Ä’•’≠’°’∂’•÷Ä’´ ’ø’∏’∂ ’ß÷Ä÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’∂’∏÷Ä’°’±÷á’∏÷Ç’©’µ’°’∂ ÷Å’∏÷Ç÷Å’°’§÷Ä’∏÷Ç’©’µ’∏÷Ç’∂÷â\",\n",
        "    \"‘≥’µ’∏÷Ç’≤’°’Ø’°’∂ ’ø’∏’∂’°’æ’°’≥’°’º’® ’∑’°’ø ’¥’°÷Ä’§ ’ß÷Ä ’∞’°’æ’°÷Ñ’•’¨÷â\",\n",
        "    \"‘ø’•’∂’ø÷Ä’∏’∂’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’¥’°÷Ä’¶’°’Ø’°’∂ ’¥’´’ª’∏÷Å’°’º’∏÷Ç’¥÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’´ ’©’°’ø÷Ä’∏’∂’∏÷Ç’¥ ’∂’∏÷Ä ’∂’•÷Ä’Ø’°’µ’°÷Å’∏÷Ç’¥ ’ß÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’°’æ’°’∂’§’°’Ø’°’∂ ’∫’°÷Ä’•÷Ä’´ ’∞’°’¥’•÷Ä’£÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’¢’°÷Å’æ’•÷Å ’∂’∏÷Ä ’Ω÷Ä’≥’°÷Ä’°’∂÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’¨’∏÷Ç’Ω’°’∂’Ø’°÷Ä’π’°’Ø’°’∂ ’¥÷Ä÷Å’∏÷Ç’µ’©÷â\",\n",
        "    \"’ç÷á’°’∂’∏÷Ç’¥ ’¶’¢’∏’Ω’°’∑÷Ä’ª’´’Ø’∂’•÷Ä’® ’∑’°’ø’°÷Å’•’¨ ’•’∂÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’∂’Ø’°÷Ä’π’°’Ø’°’∂ ÷Å’∏÷Ç÷Å’°’∞’°’∂’§’•’Ω÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’¢’°÷Å’æ’•÷Å ’∂’∏÷Ä ’Ø’´’∂’∏’©’°’ø÷Ä’∏’∂÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’µ’´’∂ ’°’µ’£’∏÷Ç’¥ ’¢’°÷Å’æ’•÷Å ’°’¥’°’º’°’µ’´’∂ ’Ω÷Ä’≥’°÷Ä’°’∂÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’ø’∏’∂’°’Ø’°’∂ ’∑÷Ñ’•÷Ä’©÷â\",\n",
        "    \"‘≥’•’≤’°÷Ä÷Ñ’∏÷Ç’∂’´÷Ñ’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ÷É’°’º’°’ø’∏’∂÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂’∏÷Ç’¥ ’•’≤’°’∂’°’Ø’® ’¢’°÷Ä’•’¨’°’æ’æ’•’¨ ’ß÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’•÷Ä’•’≠’°’∂’•÷Ä’´ ÷Å’∏÷Ç÷Å’°’∞’°’∂’§’•’Ω÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’¢’°÷Å’æ’•÷Å ’ø’•’≠’∂’∏’¨’∏’£’´’°’Ø’°’∂ ’Ø’•’∂’ø÷Ä’∏’∂÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’£’°÷Ä’∂’°’∂’°’µ’´’∂ ÷Å’∏÷Ç÷Å’°’∞’°’∂’§’•’Ω÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’∞’•’Æ’°’∂’æ’°’µ’´’∂ ’°÷Ä’∑’°’æ÷â\",\n",
        "    \"‘≥’µ’∏÷Ç’≤’•÷Ä’∏÷Ç’¥ ’Ω’Ø’Ω’æ’•’¨ ’ß ’°’∑’∂’°’∂ ’¢’•÷Ä÷Ñ’°’∞’°’æ’°÷Ñ’®÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’µ’´’∂ ’°’µ’£’∏÷Ç’¥ ’¢’°÷Å’æ’•÷Å ’∂’∏÷Ä ’Æ’°’≤’Ø’°’∂’∏÷Å÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’°’∂÷Å’Ø’°÷Å’æ’•÷Å ’£÷Ä’°’Ø’°’∂’∏÷Ç’©’µ’°’∂ ÷Ö÷Ä÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’°’æ’°’∂’§’°’Ø’°’∂ ÷É’°’º’°’ø’∏’∂÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’¢’°÷Å’æ’•÷Å ’°÷Ä’æ’•’Ω’ø’´ ’Ω÷Ä’°’∞÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂’∏÷Ç’¥ ’¥’∑’°’Ø’∏÷Ç’©’°’µ’´’∂ ’¥’´’ª’∏÷Å’°’º’∏÷Ç’¥’∂’•÷Ä’® ’∑’°’ø’°÷Å’•’¨ ’•’∂÷â\",\n",
        "    \"‘≥’µ’∏÷Ç’≤’•÷Ä’∏÷Ç’¥ ’´÷Ä’°’Ø’°’∂’°÷Å’æ’∏÷Ç’¥ ’•’∂ ’ø’∏’∂’°’Ø’°’∂ ’¥’´’ª’∏÷Å’°’º’∏÷Ç’¥’∂’•÷Ä÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’°’¥’°’º’°’µ’´’∂ ’¢’°÷Å÷Ö’©’µ’° ’∞’°’¥’•÷Ä’£÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’¢’°÷Å’æ’•÷Å ’∂’∏÷Ä ’£’´’ø’°’Ø’°’∂ ’Ø’•’∂’ø÷Ä’∏’∂÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’•÷Ä’´’ø’°’Ω’°÷Ä’§’°’Ø’°’∂ ’∞’°’¥’°’™’∏’≤’∏’æ÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂’∏÷Ç’¥ ’¥’°÷Ä’¶’°’Ø’°’∂ ’Ø’µ’°’∂÷Ñ’® ’¶’°÷Ä’£’°’∂’∏÷Ç’¥ ’ß÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’´ ’©’°’ø÷Ä’∏’∂’∏÷Ç’¥ ’∂’∏÷Ä ’¢’•’¥’°’§÷Ä’∏÷Ç’©’µ’∏÷Ç’∂ ’ß÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’∂’∏÷Ä’°’±÷á’∏÷Ç’©’µ’°’∂ ÷Å’∏÷Ç÷Å’°’∞’°’∂’§’•’Ω÷â\",\n",
        "    \"‘µ÷Ä÷á’°’∂’∏÷Ç’¥ ’¢’°÷Å’æ’•÷Å ’∂’∏÷Ä ’°’µ’£’´÷â\",\n",
        "    \"’è’•’≤’´ ’∏÷Ç’∂’•÷Å’°’æ ’•÷Ä’´’ø’°’Ω’°÷Ä’§’°’Ø’°’∂ ÷É’°’º’°’ø’∏’∂÷â\",\n",
        "    \"’Ä’°’∂÷Ä’°’∫’•’ø’∏÷Ç’©’µ’∏÷Ç’∂’∏÷Ç’¥ ’•’≤’°’∂’°’Ø’® ’ø’°÷Ñ’°÷Å’•’¨ ’ß÷â\"\n",
        "]\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    \"text\": propaganda_texts + neutral_texts,\n",
        "    \"label\": [1] * len(propaganda_texts) + [0] * len(neutral_texts)\n",
        "})\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=1000,\n",
        "    ngram_range=(1, 2),\n",
        "    token_pattern=r'\\b[’°-÷Ü‘±-’ñ]{2,}\\b'\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(data[\"text\"])\n",
        "y = data[\"label\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "model = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    depth=5,\n",
        "    learning_rate=0.03,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"=== ‘≥’∂’°’∞’°’ø’¥’°’∂ ‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ’∂’•÷Ä ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"’â’•’¶’∏÷Ñ\", \"’î’°÷Ä’∏’¶’π’°’Ø’°’∂\"]))\n",
        "print(\"’É’∑’£÷Ä’ø’∏÷Ç’©’µ’∏÷Ç’∂:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "joblib.dump({\"model\": model, \"vectorizer\": vectorizer}, \"armenian_propaganda_detector.pkl\")\n",
        "print(\"\\n’Ñ’∏’§’•’¨’® ’∫’°’∞’∫’°’∂’æ’•’¨ ’ß armenian_propaganda_detector.pkl ÷Ü’°’µ’¨’∏÷Ç’¥÷â\")\n"
      ],
      "metadata": {
        "id": "t8p45DE-Q4dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "model_data = joblib.load(\"armenian_propaganda_detector.pkl\")\n",
        "model = model_data[\"model\"]\n",
        "vectorizer = model_data[\"vectorizer\"]\n",
        "\n",
        "def predict_sentence(sentence: str):\n",
        "    \"\"\"’é’•÷Ä’°’§’°÷Ä’±’∂’∏÷Ç’¥ ’ß '’î’°÷Ä’∏’¶’π’°’Ø’°’∂' ’Ø’°’¥ '’â’•’¶’∏÷Ñ' ’ø’•÷Ñ’Ω’ø’´ ’§’°’Ω’®÷â\"\"\"\n",
        "    X = vectorizer.transform([sentence])\n",
        "    pred = model.predict(X)[0]\n",
        "    return \"’î’°÷Ä’∏’¶’π’°’Ø’°’∂\" if pred == 1 else \"’â’•’¶’∏÷Ñ\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sentence = input(\"‘≥÷Ä’´÷Ä ’∂’°’≠’°’§’°’Ω’∏÷Ç’©’µ’∏÷Ç’∂’® ’Ω’ø’∏÷Ç’£’•’¨’∏÷Ç ’∞’°’¥’°÷Ä: \")\n",
        "    result = predict_sentence(sentence)\n",
        "    print(f\"‘±÷Ä’§’µ’∏÷Ç’∂÷Ñ: {result}\")\n"
      ],
      "metadata": {
        "id": "K-QySZzgSivd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets sentencepiece evaluate nltk\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "import random\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Use the existing data DataFrame from the notebook\n",
        "# Assuming 'data' DataFrame with 'text' and 'label' columns exists from previous cells.\n",
        "# If not, you might need to load or define it here.\n",
        "# For example:\n",
        "# data = pd.read_csv(\"your_dataset.csv\")\n",
        "\n",
        "# Add the clean_text function\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Apply cleaning\n",
        "data['cleaned_text'] = data['text'].apply(clean_text)\n",
        "\n",
        "# Split data and save to csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42, stratify=data['label'])\n",
        "train_df.to_csv(\"train.csv\", index=False)\n",
        "test_df.to_csv(\"test.csv\", index=False)\n",
        "\n",
        "# Load the dataset from the saved csv files\n",
        "dataset = load_dataset(\"csv\", data_files={\"train\": \"train.csv\", \"test\": \"test.csv\"})\n",
        "\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def synonym_replacement(text, n=1):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set(words))\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = wordnet.synsets(random_word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def augment_examples(example):\n",
        "    # Apply augmentation to the cleaned text\n",
        "    if random.random() < 0.3:\n",
        "        example[\"cleaned_text\"] = synonym_replacement(example[\"cleaned_text\"])\n",
        "    return example\n",
        "\n",
        "# Apply augmentation on the cleaned text column\n",
        "augmented_train = dataset[\"train\"].map(augment_examples)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "def tokenize(batch):\n",
        "    # Tokenize the cleaned text\n",
        "    return tokenizer(batch[\"cleaned_text\"], truncation=True)\n",
        "\n",
        "tokenized_train = augmented_train.map(tokenize, batched=True)\n",
        "tokenized_test = dataset[\"test\"].map(tokenize, batched=True)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=2)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(-1)\n",
        "    return accuracy.compute(predictions=preds, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./AzgIntel-XLMR\")\n",
        "tokenizer.save_pretrained(\"./AzgIntel-XLMR\")"
      ],
      "metadata": {
        "id": "GuUADMqJUJLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ’ä’°’∞’°’∂’ª’æ’∏’≤ ’£÷Ä’°’§’°÷Ä’°’∂’∂’•÷Ä’®\n",
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# ’Ü’•÷Ä’¢’•’º’∂’∏÷Ç’¥ ’•’∂÷Ñ azcorpus_v0 dataset-’®\n",
        "dataset = load_dataset(\"azcorpus/azcorpus_v0\")\n",
        "\n",
        "# ’è’•’Ω’∂’•’∂÷Ñ ’´’∂’π ’•’∂’©’°’¢’°’™’´’∂’∂’•÷Ä ’∏÷Ç’∂’´\n",
        "print(dataset)\n",
        "\n",
        "# ’ï÷Ä’´’∂’°’Ø ’æ’•÷Ä÷Å’∂’•’∂÷Ñ ’¨’∏÷Ç÷Ä’•÷Ä’´ ’¢’°’™’´’∂’® (news)\n",
        "news_data = dataset[\"news\"]\n",
        "\n",
        "# ‘¥’´’ø’•’∂÷Ñ ’°’º’°’ª’´’∂ ’¥’´ ÷Ñ’°’∂’´ ’¨’∏÷Ç÷Ä’•÷Ä’®\n",
        "print(news_data[0])\n",
        "\n",
        "# ’é’•÷Ä’°÷É’∏’≠’•’∂÷Ñ pandas DataFrame-’´, ’∏÷Ä’∫’•’Ω’¶’´ ’∞’•’∑’ø ÷Ü’´’¨’ø÷Ä’•’∂÷Ñ\n",
        "df = pd.DataFrame(news_data)\n",
        "\n",
        "# ’ñ’´’¨’ø÷Ä’•’∂÷Ñ ’¥’´’°’µ’∂ ÷Ñ’°’≤’°÷Ñ’°’Ø’°’∂ ’¨’∏÷Ç÷Ä’•÷Ä’®\n",
        "# ’°’µ’Ω’ø’•’≤ \"siyas…ôt\" (÷Ñ’°’≤’°÷Ñ’°’Ø’°’∂’∏÷Ç’©’µ’∏÷Ç’∂) ’∞’´’¥’∂’°’Ø’°’∂ keyword-’∂ ’ß ’°’§÷Ä’¢’•’ª’°’∂’•÷Ä’•’∂’∏÷Ç’¥\n",
        "politics_df = df[df[\"text\"].str.contains(\"siyas…ôt|politika|parlament|prezident|hakimiyy…ôt\", case=False, na=False)]\n",
        "\n",
        "# ’è’•’Ω’∂’•’∂÷Ñ ÷Ñ’°’∂’´ ’∞’∏’§’æ’°’Æ ’Ω’ø’°÷Å’°’∂÷Ñ\n",
        "print(\"’î’°’≤’°÷Ñ’°’Ø’°’∂ ’¨’∏÷Ç÷Ä’•÷Ä’´ ÷Ñ’°’∂’°’Ø:\", len(politics_df))\n",
        "\n",
        "# ’ä’°’∞’∫’°’∂’•’∂÷Ñ CSV ’±÷á’°’π’°÷É’∏’æ’ù ’∞’•’ø’°’£’° ÷Ö’£’ø’°’£’∏÷Ä’Æ’¥’°’∂ ’∞’°’¥’°÷Ä\n",
        "politics_df.to_csv(\"azerbaijani_political_news.csv\", index=False, encoding=\"utf-8-sig\")\n"
      ],
      "metadata": {
        "id": "qO71eJWZIJzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub datasets\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "# ‘ø’∫÷Å÷Ä’∏÷Ç ÷Ñ’∏ HF access token-’® ’°’µ’Ω’ø’•’≤\n",
        "login(\"hf_xxxxxxxxxxxxxxxxxxxxxxxxx\")\n"
      ],
      "metadata": {
        "id": "-CPPWoamM-Fd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}